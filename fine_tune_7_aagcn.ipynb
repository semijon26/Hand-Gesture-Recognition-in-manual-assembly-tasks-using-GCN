{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-17T11:05:53.884052200Z",
     "start_time": "2024-01-17T11:05:49.462461200Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from model import aagcn, loss, SAM\n",
    "from utils import adj_mat, training_supervision\n",
    "from data.handpose_dataset import HandPoseDatasetNumpy, df_to_numpy\n",
    "from data.get_data_from_csv import get_train_data, get_val_data\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn\n",
    "from config_fine_tuning_occluded_hand_detection import CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e18a904b2db6ab6a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": "Model(\n  (data_bn): BatchNorm1d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (l1): TCN_GCN_unit(\n    (gcn1): unit_gcn(\n      (conv_d): ModuleList(\n        (0-2): 3 x Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_a): ModuleList(\n        (0-2): 3 x Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_b): ModuleList(\n        (0-2): 3 x Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_ta): Conv1d(64, 1, kernel_size=(9,), stride=(1,), padding=(4,))\n      (conv_sa): Conv1d(64, 1, kernel_size=(21,), stride=(1,), padding=(10,))\n      (fc1c): Linear(in_features=64, out_features=32, bias=True)\n      (fc2c): Linear(in_features=32, out_features=64, bias=True)\n      (down): Sequential(\n        (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (soft): Softmax(dim=-2)\n      (tan): Tanh()\n      (sigmoid): Sigmoid()\n      (relu): ReLU(inplace=True)\n    )\n    (tcn1): unit_tcn(\n      (conv): Conv2d(64, 64, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0))\n      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (l2): TCN_GCN_unit(\n    (gcn1): unit_gcn(\n      (conv_d): ModuleList(\n        (0-2): 3 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_a): ModuleList(\n        (0-2): 3 x Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_b): ModuleList(\n        (0-2): 3 x Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_ta): Conv1d(64, 1, kernel_size=(9,), stride=(1,), padding=(4,))\n      (conv_sa): Conv1d(64, 1, kernel_size=(21,), stride=(1,), padding=(10,))\n      (fc1c): Linear(in_features=64, out_features=32, bias=True)\n      (fc2c): Linear(in_features=32, out_features=64, bias=True)\n      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (soft): Softmax(dim=-2)\n      (tan): Tanh()\n      (sigmoid): Sigmoid()\n      (relu): ReLU(inplace=True)\n    )\n    (tcn1): unit_tcn(\n      (conv): Conv2d(64, 64, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0))\n      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (l3): TCN_GCN_unit(\n    (gcn1): unit_gcn(\n      (conv_d): ModuleList(\n        (0-2): 3 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_a): ModuleList(\n        (0-2): 3 x Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_b): ModuleList(\n        (0-2): 3 x Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_ta): Conv1d(64, 1, kernel_size=(9,), stride=(1,), padding=(4,))\n      (conv_sa): Conv1d(64, 1, kernel_size=(21,), stride=(1,), padding=(10,))\n      (fc1c): Linear(in_features=64, out_features=32, bias=True)\n      (fc2c): Linear(in_features=32, out_features=64, bias=True)\n      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (soft): Softmax(dim=-2)\n      (tan): Tanh()\n      (sigmoid): Sigmoid()\n      (relu): ReLU(inplace=True)\n    )\n    (tcn1): unit_tcn(\n      (conv): Conv2d(64, 64, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0))\n      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (l4): TCN_GCN_unit(\n    (gcn1): unit_gcn(\n      (conv_d): ModuleList(\n        (0-2): 3 x Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_a): ModuleList(\n        (0-2): 3 x Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_b): ModuleList(\n        (0-2): 3 x Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_ta): Conv1d(64, 1, kernel_size=(9,), stride=(1,), padding=(4,))\n      (conv_sa): Conv1d(64, 1, kernel_size=(21,), stride=(1,), padding=(10,))\n      (fc1c): Linear(in_features=64, out_features=32, bias=True)\n      (fc2c): Linear(in_features=32, out_features=64, bias=True)\n      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (soft): Softmax(dim=-2)\n      (tan): Tanh()\n      (sigmoid): Sigmoid()\n      (relu): ReLU(inplace=True)\n    )\n    (tcn1): unit_tcn(\n      (conv): Conv2d(64, 64, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0))\n      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (l5): TCN_GCN_unit(\n    (gcn1): unit_gcn(\n      (conv_d): ModuleList(\n        (0-2): 3 x Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_a): ModuleList(\n        (0-2): 3 x Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_b): ModuleList(\n        (0-2): 3 x Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_ta): Conv1d(128, 1, kernel_size=(9,), stride=(1,), padding=(4,))\n      (conv_sa): Conv1d(128, 1, kernel_size=(21,), stride=(1,), padding=(10,))\n      (fc1c): Linear(in_features=128, out_features=64, bias=True)\n      (fc2c): Linear(in_features=64, out_features=128, bias=True)\n      (down): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (soft): Softmax(dim=-2)\n      (tan): Tanh()\n      (sigmoid): Sigmoid()\n      (relu): ReLU(inplace=True)\n    )\n    (tcn1): unit_tcn(\n      (conv): Conv2d(128, 128, kernel_size=(9, 1), stride=(2, 1), padding=(4, 0))\n      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (relu): ReLU(inplace=True)\n    (residual): unit_tcn(\n      (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 1))\n      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (l6): TCN_GCN_unit(\n    (gcn1): unit_gcn(\n      (conv_d): ModuleList(\n        (0-2): 3 x Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_a): ModuleList(\n        (0-2): 3 x Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_b): ModuleList(\n        (0-2): 3 x Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_ta): Conv1d(128, 1, kernel_size=(9,), stride=(1,), padding=(4,))\n      (conv_sa): Conv1d(128, 1, kernel_size=(21,), stride=(1,), padding=(10,))\n      (fc1c): Linear(in_features=128, out_features=64, bias=True)\n      (fc2c): Linear(in_features=64, out_features=128, bias=True)\n      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (soft): Softmax(dim=-2)\n      (tan): Tanh()\n      (sigmoid): Sigmoid()\n      (relu): ReLU(inplace=True)\n    )\n    (tcn1): unit_tcn(\n      (conv): Conv2d(128, 128, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0))\n      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (relu): ReLU(inplace=True)\n  )\n  (l7): TCN_GCN_unit(\n    (gcn1): unit_gcn(\n      (conv_d): ModuleList(\n        (0-2): 3 x Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_a): ModuleList(\n        (0-2): 3 x Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_b): ModuleList(\n        (0-2): 3 x Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (conv_ta): Conv1d(128, 1, kernel_size=(9,), stride=(1,), padding=(4,))\n      (conv_sa): Conv1d(128, 1, kernel_size=(21,), stride=(1,), padding=(10,))\n      (fc1c): Linear(in_features=128, out_features=64, bias=True)\n      (fc2c): Linear(in_features=64, out_features=128, bias=True)\n      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (soft): Softmax(dim=-2)\n      (tan): Tanh()\n      (sigmoid): Sigmoid()\n      (relu): ReLU(inplace=True)\n    )\n    (tcn1): unit_tcn(\n      (conv): Conv2d(128, 128, kernel_size=(9, 1), stride=(2, 1), padding=(4, 0))\n      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (relu): ReLU(inplace=True)\n    (residual): unit_tcn(\n      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 1))\n      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (fc): Linear(in_features=128, out_features=6, bias=True)\n  (drop_out): Dropout(p=0.5, inplace=False)\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'trained_models/7_AAGCN_Focal_seqlen32_release_SAM_joints1_joints2_oridist/f10.8142688679245284_valloss310.2437744140625_epoch13.pth'\n",
    "\n",
    "graph = aagcn.Graph(adj_mat.num_node, adj_mat.self_link, adj_mat.inward, adj_mat.outward, adj_mat.neighbor)\n",
    "\n",
    "model = aagcn.Model(num_class=CFG.num_classes, num_point=21, num_person=1, graph=graph, drop_out=0.5,\n",
    "                    in_channels=CFG.num_feats)\n",
    "\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T11:05:57.216372200Z",
     "start_time": "2024-01-17T11:05:56.855580600Z"
    }
   },
   "id": "2169ad9f860d1525"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modify model -> only the classes we want for our model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11b0dce12eb5a46"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-2.1048e-02,  1.8790e+00, -2.7541e-01,  1.7177e+00, -3.2310e-01,\n         -1.1365e-01,  8.6734e-01, -2.8909e-01,  4.4176e-01, -5.0910e-02,\n         -4.2640e-01,  6.8933e-01, -1.3862e+00, -9.0482e-01,  4.7733e-01,\n          1.1812e-01, -1.4559e+00,  5.4456e-01, -9.0954e-01, -7.7671e-02,\n         -1.1162e+00,  1.1737e+00, -4.6457e-01,  1.6353e+00, -9.6110e-02,\n          5.2284e-01, -1.7120e+00,  6.5424e-01,  2.5135e-01, -1.2197e+00,\n         -1.4798e+00,  3.2154e-01,  5.9754e-01, -1.3115e-01,  1.9135e-01,\n          2.8214e-04, -2.1805e-01,  2.1576e+00,  7.3687e-01,  1.2417e+00,\n         -8.6287e-01, -3.9431e-01,  1.0210e+00, -5.1125e-01,  1.3798e+00,\n          1.4952e-01, -1.7389e+00, -2.6711e-01,  4.8478e-01, -8.2077e-01,\n          5.5378e-01,  1.7504e-01,  5.8467e-01,  1.4706e+00, -3.0760e-02,\n          1.9243e-01, -5.7288e-02,  5.9171e-01, -4.3727e-01, -1.8782e+00,\n         -1.9036e+00, -7.2853e-01, -1.0704e+00,  4.9167e-01, -2.0546e+00,\n          7.2889e-01,  1.8045e+00, -3.2555e-01, -5.7583e-01, -8.0613e-01,\n         -2.5154e-01, -1.4652e-01, -1.4979e+00,  5.6577e-01, -1.1391e+00,\n         -6.6149e-01, -1.1485e-01, -1.9795e-01,  3.2825e-01,  2.6204e-01,\n         -3.2781e-01,  1.1687e+00,  3.6500e-01, -1.7321e-01, -9.7797e-01,\n          1.0494e-01,  1.8278e+00, -1.7894e+00,  1.5786e+00,  1.1960e+00,\n          1.3977e+00,  5.5563e-01,  6.7158e-02, -5.4301e-01,  1.1025e+00,\n          4.7601e-01,  4.6535e-01,  3.7977e-01,  1.1363e+00,  3.6294e-01,\n         -4.9640e-01, -3.5893e-01, -7.2202e-01, -1.3517e-01,  4.6112e-01,\n         -4.5037e-02, -1.4409e-01, -1.5757e+00, -7.0928e-02,  1.3150e+00,\n         -1.6943e+00,  3.9646e-01,  3.9307e-01, -8.6641e-01, -1.7031e+00,\n         -6.1335e-01,  3.8213e-01, -1.8950e-01,  2.1084e+00,  7.4286e-01,\n          9.8948e-01,  1.4336e+00, -1.5128e+00, -8.0864e-01, -2.7269e-01,\n          7.2434e-01, -2.2215e-01,  1.4311e+00],\n        [ 5.7598e-01, -1.2449e+00,  3.2492e-01, -5.7526e-01, -1.6044e+00,\n         -9.3897e-01, -1.2600e+00,  7.1301e-01,  1.7941e+00,  4.8840e-01,\n          5.3875e-01,  6.5052e-01,  7.8452e-01,  3.9712e-01, -6.8590e-01,\n          1.2345e+00,  1.6633e-01, -3.1530e+00,  7.7918e-01,  5.8488e-01,\n          5.7609e-01,  1.1552e+00,  6.1463e-01,  1.5005e+00,  1.7392e+00,\n          3.6954e-02, -1.5493e-01,  1.1346e+00, -2.3822e-01,  1.3584e-01,\n          9.8518e-01,  6.5151e-01, -9.7399e-01, -1.5191e+00,  8.1261e-01,\n          8.7438e-01,  5.1940e-01, -1.6304e+00,  1.2099e+00, -1.6034e+00,\n          1.4508e+00, -8.6442e-01,  6.5091e-01,  5.2234e-01, -9.8505e-01,\n          2.4199e+00,  1.7180e+00,  2.0734e+00, -2.0408e+00, -3.6238e-01,\n         -6.3789e-01,  1.3142e-01, -3.4216e-01,  2.8767e-01, -1.1245e-01,\n          8.3575e-01,  1.0651e+00,  1.3866e+00, -4.7377e-01, -9.6715e-01,\n          2.0161e+00,  2.6484e-01,  5.2841e-01, -5.3045e-01, -6.5691e-01,\n         -8.2351e-01,  1.0743e+00,  1.2597e+00, -1.5940e+00,  4.3376e-01,\n         -1.3471e+00, -1.1754e+00, -4.4784e-01,  1.1230e-01, -2.1708e+00,\n         -9.6546e-01, -6.3012e-01,  9.4375e-01, -2.3529e-01,  2.0210e+00,\n          2.2553e+00,  1.9630e+00, -1.7020e+00, -3.2047e-01,  1.3720e+00,\n          6.5450e-01,  7.8594e-01, -2.5736e+00, -1.2300e+00,  2.0482e-01,\n         -2.3460e+00,  1.6180e+00,  5.6342e-01,  6.8383e-01,  9.0433e-01,\n          1.5410e+00,  7.7652e-01, -2.3429e-01,  1.0056e+00,  1.2026e+00,\n          1.4383e+00, -1.3222e+00,  4.6259e-01,  1.0594e+00, -1.2570e-01,\n          6.2270e-01, -2.6614e-01, -5.1944e-01,  1.4849e+00, -2.0100e+00,\n          3.3553e-01, -1.1841e+00, -5.8619e-01,  8.4678e-01,  1.3010e+00,\n         -2.2055e+00,  7.5814e-01, -1.2102e+00,  3.8502e-01,  2.6428e-03,\n         -3.2560e-01,  9.3539e-01, -1.0264e+00,  6.5855e-01,  8.8483e-01,\n          6.8857e-01, -1.1910e+00,  6.2854e-01]], device='cuda:0',\n       requires_grad=True)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change last layer\n",
    "model.fc = nn.Linear(128, CFG.num_classes_fine_tuning).to(device)\n",
    "\n",
    "# init the new layer\n",
    "nn.init.normal_(model.fc.weight, 0, math.sqrt(2. / CFG.num_classes_fine_tuning))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T11:05:59.181858800Z",
     "start_time": "2024-01-17T11:05:59.177223100Z"
    }
   },
   "id": "dc1758e49dcac320"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9eac5679f84793c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TRAIN DATA DISTRIBUTION\n",
      "LABEL\n",
      "Wave        2208\n",
      "ThumbsUp    2112\n",
      "Name: count, dtype: int64\n",
      "[INFO] TEST DATA DISTRIBUTION\n",
      "LABEL\n",
      "Wave        1120\n",
      "ThumbsUp     736\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "                 WRIST           THUMB_CMC           THUMB_MCP  \\\n0   (0.35, 1.23, 0.11)  (0.32, 1.24, 0.11)  (0.27, 1.26, 0.11)   \n1   (0.34, 1.24, 0.11)  (0.31, 1.24, 0.11)  (0.27, 1.26, 0.10)   \n2   (0.34, 1.24, 0.11)  (0.31, 1.25, 0.11)  (0.27, 1.27, 0.10)   \n3   (0.37, 1.24, 0.11)  (0.35, 1.25, 0.12)  (0.30, 1.27, 0.11)   \n4   (0.39, 1.25, 0.12)  (0.36, 1.27, 0.13)  (0.33, 1.30, 0.13)   \n5   (0.38, 1.27, 0.13)  (0.35, 1.29, 0.13)  (0.33, 1.32, 0.14)   \n6   (0.37, 1.27, 0.12)  (0.34, 1.28, 0.13)  (0.31, 1.31, 0.14)   \n7   (0.34, 1.25, 0.12)  (0.31, 1.26, 0.12)  (0.27, 1.29, 0.13)   \n8   (0.33, 1.24, 0.11)  (0.30, 1.25, 0.12)  (0.26, 1.27, 0.12)   \n9   (0.31, 1.23, 0.12)  (0.28, 1.24, 0.11)  (0.24, 1.25, 0.10)   \n10  (0.31, 1.24, 0.12)  (0.28, 1.24, 0.12)  (0.24, 1.25, 0.10)   \n11  (0.31, 1.24, 0.13)  (0.29, 1.24, 0.12)  (0.24, 1.26, 0.11)   \n12  (0.33, 1.26, 0.13)  (0.30, 1.26, 0.13)  (0.26, 1.28, 0.13)   \n13  (0.35, 1.26, 0.13)  (0.32, 1.27, 0.14)  (0.29, 1.30, 0.15)   \n14  (0.35, 1.26, 0.13)  (0.32, 1.27, 0.14)  (0.30, 1.31, 0.15)   \n15  (0.34, 1.25, 0.13)  (0.31, 1.27, 0.14)  (0.29, 1.31, 0.15)   \n16  (0.32, 1.25, 0.13)  (0.29, 1.27, 0.13)  (0.26, 1.29, 0.14)   \n17  (0.30, 1.24, 0.12)  (0.27, 1.25, 0.13)  (0.23, 1.27, 0.12)   \n18  (0.29, 1.23, 0.12)  (0.26, 1.23, 0.11)  (0.22, 1.25, 0.11)   \n19  (0.28, 1.22, 0.11)  (0.25, 1.22, 0.11)  (0.21, 1.23, 0.10)   \n\n              THUMB_IP           THUMB_TIP    INDEX_FINGER_MCP  \\\n0   (0.25, 1.28, 0.11)  (0.25, 1.29, 0.11)  (0.29, 1.31, 0.11)   \n1   (0.25, 1.27, 0.10)  (0.24, 1.29, 0.10)  (0.28, 1.31, 0.11)   \n2   (0.25, 1.29, 0.10)  (0.24, 1.30, 0.11)  (0.28, 1.31, 0.11)   \n3   (0.29, 1.28, 0.11)  (0.28, 1.30, 0.11)  (0.32, 1.31, 0.11)   \n4   (0.31, 1.32, 0.13)  (0.31, 1.33, 0.13)  (0.36, 1.33, 0.13)   \n5   (0.31, 1.34, 0.14)  (0.31, 1.35, 0.15)  (0.35, 1.35, 0.14)   \n6   (0.30, 1.33, 0.14)  (0.30, 1.35, 0.15)  (0.34, 1.35, 0.14)   \n7   (0.26, 1.30, 0.13)  (0.25, 1.32, 0.14)  (0.30, 1.33, 0.13)   \n8   (0.24, 1.29, 0.12)  (0.24, 1.30, 0.13)  (0.27, 1.31, 0.12)   \n9   (0.22, 1.27, 0.10)  (0.21, 1.28, 0.11)  (0.24, 1.30, 0.11)   \n10  (0.21, 1.26, 0.10)  (0.20, 1.27, 0.11)  (0.24, 1.29, 0.11)   \n11  (0.22, 1.27, 0.11)  (0.21, 1.28, 0.11)  (0.25, 1.30, 0.11)   \n12  (0.25, 1.30, 0.13)  (0.24, 1.31, 0.14)  (0.28, 1.32, 0.13)   \n13  (0.28, 1.32, 0.15)  (0.27, 1.33, 0.16)  (0.31, 1.34, 0.15)   \n14  (0.29, 1.33, 0.16)  (0.29, 1.34, 0.17)  (0.33, 1.34, 0.15)   \n15  (0.28, 1.33, 0.16)  (0.28, 1.34, 0.17)  (0.32, 1.34, 0.15)   \n16  (0.25, 1.31, 0.15)  (0.24, 1.33, 0.16)  (0.29, 1.33, 0.15)   \n17  (0.22, 1.29, 0.13)  (0.22, 1.31, 0.14)  (0.25, 1.31, 0.13)   \n18  (0.20, 1.26, 0.11)  (0.19, 1.27, 0.12)  (0.23, 1.29, 0.11)   \n19  (0.19, 1.25, 0.10)  (0.18, 1.26, 0.10)  (0.22, 1.28, 0.10)   \n\n      INDEX_FINGER_PIP    INDEX_FINGER_DIP    INDEX_FINGER_TIP  \\\n0   (0.27, 1.33, 0.12)  (0.26, 1.35, 0.12)  (0.25, 1.36, 0.13)   \n1   (0.26, 1.33, 0.11)  (0.24, 1.34, 0.11)  (0.23, 1.36, 0.12)   \n2   (0.27, 1.34, 0.11)  (0.25, 1.35, 0.12)  (0.24, 1.37, 0.12)   \n3   (0.30, 1.33, 0.11)  (0.29, 1.35, 0.12)  (0.28, 1.36, 0.13)   \n4   (0.35, 1.36, 0.13)  (0.34, 1.38, 0.14)  (0.34, 1.39, 0.15)   \n5   (0.34, 1.38, 0.15)  (0.34, 1.40, 0.16)  (0.34, 1.41, 0.16)   \n6   (0.33, 1.38, 0.15)  (0.33, 1.40, 0.16)  (0.32, 1.41, 0.16)   \n7   (0.28, 1.35, 0.14)  (0.28, 1.37, 0.14)  (0.27, 1.38, 0.15)   \n8   (0.26, 1.34, 0.13)  (0.25, 1.36, 0.13)  (0.24, 1.37, 0.14)   \n9   (0.23, 1.32, 0.11)  (0.21, 1.33, 0.11)  (0.20, 1.34, 0.12)   \n10  (0.22, 1.31, 0.11)  (0.21, 1.32, 0.11)  (0.20, 1.33, 0.12)   \n11  (0.23, 1.32, 0.11)  (0.22, 1.33, 0.12)  (0.21, 1.34, 0.13)   \n12  (0.27, 1.35, 0.14)  (0.26, 1.36, 0.14)  (0.25, 1.37, 0.15)   \n13  (0.31, 1.37, 0.15)  (0.30, 1.38, 0.16)  (0.30, 1.40, 0.17)   \n14  (0.33, 1.37, 0.16)  (0.33, 1.38, 0.17)  (0.32, 1.39, 0.18)   \n15  (0.32, 1.37, 0.16)  (0.32, 1.38, 0.17)  (0.31, 1.39, 0.18)   \n16  (0.28, 1.36, 0.15)  (0.28, 1.38, 0.16)  (0.27, 1.39, 0.17)   \n17  (0.24, 1.34, 0.14)  (0.24, 1.35, 0.15)  (0.23, 1.36, 0.15)   \n18  (0.22, 1.31, 0.11)  (0.20, 1.33, 0.12)  (0.19, 1.34, 0.13)   \n19  (0.20, 1.30, 0.10)  (0.19, 1.31, 0.11)  (0.18, 1.31, 0.11)   \n\n     MIDDLE_FINGER_MCP  ...   MIDDLE_FINGER_TIP     RING_FINGER_MCP  \\\n0   (0.30, 1.31, 0.12)  ...  (0.27, 1.39, 0.13)  (0.32, 1.31, 0.13)   \n1   (0.29, 1.31, 0.11)  ...  (0.25, 1.38, 0.12)  (0.31, 1.32, 0.12)   \n2   (0.30, 1.32, 0.11)  ...  (0.26, 1.39, 0.13)  (0.31, 1.32, 0.12)   \n3   (0.33, 1.32, 0.11)  ...  (0.30, 1.39, 0.13)  (0.35, 1.32, 0.12)   \n4   (0.37, 1.34, 0.13)  ...  (0.36, 1.41, 0.15)  (0.39, 1.33, 0.14)   \n5   (0.37, 1.35, 0.14)  ...  (0.36, 1.42, 0.17)  (0.38, 1.35, 0.15)   \n6   (0.35, 1.35, 0.14)  ...  (0.35, 1.42, 0.17)  (0.37, 1.35, 0.15)   \n7   (0.31, 1.33, 0.13)  ...  (0.30, 1.40, 0.15)  (0.33, 1.33, 0.14)   \n8   (0.29, 1.32, 0.13)  ...  (0.27, 1.39, 0.14)  (0.31, 1.32, 0.14)   \n9   (0.26, 1.30, 0.11)  ...  (0.22, 1.36, 0.12)  (0.27, 1.30, 0.13)   \n10  (0.25, 1.30, 0.11)  ...  (0.21, 1.36, 0.12)  (0.27, 1.31, 0.13)   \n11  (0.26, 1.31, 0.12)  ...  (0.22, 1.36, 0.13)  (0.28, 1.31, 0.13)   \n12  (0.29, 1.33, 0.14)  ...  (0.27, 1.39, 0.16)  (0.31, 1.33, 0.15)   \n13  (0.33, 1.34, 0.15)  ...  (0.33, 1.41, 0.18)  (0.35, 1.34, 0.16)   \n14  (0.34, 1.34, 0.16)  ...  (0.35, 1.40, 0.19)  (0.36, 1.33, 0.16)   \n15  (0.34, 1.34, 0.15)  ...  (0.34, 1.40, 0.19)  (0.35, 1.33, 0.16)   \n16  (0.30, 1.33, 0.15)  ...  (0.29, 1.40, 0.18)  (0.32, 1.33, 0.16)   \n17  (0.27, 1.32, 0.14)  ...  (0.25, 1.38, 0.16)  (0.28, 1.31, 0.14)   \n18  (0.24, 1.30, 0.12)  ...  (0.21, 1.36, 0.13)  (0.26, 1.30, 0.13)   \n19  (0.23, 1.29, 0.11)  ...  (0.19, 1.34, 0.12)  (0.24, 1.29, 0.12)   \n\n       RING_FINGER_PIP     RING_FINGER_DIP     RING_FINGER_TIP  \\\n0   (0.31, 1.35, 0.13)  (0.31, 1.37, 0.14)  (0.30, 1.38, 0.14)   \n1   (0.30, 1.35, 0.13)  (0.29, 1.37, 0.13)  (0.28, 1.38, 0.13)   \n2   (0.30, 1.36, 0.13)  (0.29, 1.38, 0.13)  (0.29, 1.39, 0.14)   \n3   (0.34, 1.35, 0.13)  (0.33, 1.37, 0.13)  (0.33, 1.38, 0.14)   \n4   (0.39, 1.37, 0.14)  (0.39, 1.38, 0.15)  (0.39, 1.40, 0.16)   \n5   (0.39, 1.38, 0.15)  (0.39, 1.40, 0.16)  (0.39, 1.41, 0.17)   \n6   (0.38, 1.38, 0.15)  (0.38, 1.40, 0.16)  (0.38, 1.41, 0.17)   \n7   (0.33, 1.36, 0.14)  (0.33, 1.38, 0.15)  (0.32, 1.39, 0.16)   \n8   (0.30, 1.35, 0.14)  (0.30, 1.37, 0.14)  (0.29, 1.38, 0.15)   \n9   (0.26, 1.33, 0.13)  (0.25, 1.35, 0.13)  (0.25, 1.36, 0.14)   \n10  (0.25, 1.33, 0.13)  (0.24, 1.35, 0.13)  (0.24, 1.36, 0.14)   \n11  (0.26, 1.34, 0.13)  (0.25, 1.36, 0.14)  (0.25, 1.37, 0.14)   \n12  (0.31, 1.36, 0.15)  (0.30, 1.38, 0.16)  (0.29, 1.39, 0.17)   \n13  (0.35, 1.37, 0.17)  (0.35, 1.38, 0.18)  (0.35, 1.39, 0.18)   \n14  (0.37, 1.36, 0.17)  (0.37, 1.38, 0.18)  (0.37, 1.39, 0.19)   \n15  (0.36, 1.36, 0.17)  (0.36, 1.37, 0.18)  (0.36, 1.38, 0.19)   \n16  (0.32, 1.36, 0.16)  (0.32, 1.37, 0.17)  (0.32, 1.39, 0.18)   \n17  (0.28, 1.34, 0.15)  (0.28, 1.36, 0.16)  (0.27, 1.37, 0.16)   \n18  (0.25, 1.33, 0.13)  (0.24, 1.35, 0.14)  (0.24, 1.36, 0.14)   \n19  (0.23, 1.32, 0.12)  (0.22, 1.33, 0.13)  (0.21, 1.34, 0.13)   \n\n             PINKY_MCP           PINKY_PIP           PINKY_DIP  \\\n0   (0.33, 1.31, 0.14)  (0.33, 1.33, 0.14)  (0.33, 1.35, 0.14)   \n1   (0.32, 1.31, 0.13)  (0.32, 1.34, 0.13)  (0.31, 1.36, 0.14)   \n2   (0.33, 1.32, 0.14)  (0.33, 1.35, 0.14)  (0.32, 1.36, 0.14)   \n3   (0.36, 1.32, 0.13)  (0.37, 1.34, 0.13)  (0.37, 1.36, 0.13)   \n4   (0.40, 1.32, 0.14)  (0.41, 1.35, 0.14)  (0.42, 1.37, 0.15)   \n5   (0.40, 1.34, 0.15)  (0.41, 1.36, 0.15)  (0.41, 1.38, 0.16)   \n6   (0.39, 1.34, 0.15)  (0.40, 1.36, 0.15)  (0.40, 1.38, 0.16)   \n7   (0.34, 1.32, 0.14)  (0.35, 1.34, 0.14)  (0.35, 1.36, 0.15)   \n8   (0.32, 1.31, 0.14)  (0.32, 1.34, 0.14)  (0.32, 1.35, 0.15)   \n9   (0.29, 1.30, 0.14)  (0.28, 1.32, 0.14)  (0.28, 1.34, 0.14)   \n10  (0.28, 1.30, 0.14)  (0.28, 1.33, 0.14)  (0.27, 1.34, 0.14)   \n11  (0.29, 1.31, 0.15)  (0.28, 1.33, 0.15)  (0.28, 1.35, 0.15)   \n12  (0.33, 1.32, 0.16)  (0.33, 1.35, 0.16)  (0.32, 1.36, 0.17)   \n13  (0.36, 1.33, 0.16)  (0.37, 1.35, 0.17)  (0.37, 1.36, 0.18)   \n14  (0.37, 1.32, 0.16)  (0.38, 1.34, 0.17)  (0.39, 1.35, 0.18)   \n15  (0.36, 1.32, 0.16)  (0.37, 1.34, 0.17)  (0.38, 1.35, 0.18)   \n16  (0.33, 1.32, 0.16)  (0.34, 1.34, 0.17)  (0.34, 1.35, 0.18)   \n17  (0.30, 1.30, 0.15)  (0.30, 1.33, 0.16)  (0.30, 1.34, 0.16)   \n18  (0.27, 1.29, 0.14)  (0.27, 1.32, 0.14)  (0.27, 1.33, 0.15)   \n19  (0.26, 1.29, 0.14)  (0.25, 1.31, 0.14)  (0.25, 1.33, 0.14)   \n\n             PINKY_TIP LABEL  \n0   (0.33, 1.37, 0.14)  Wave  \n1   (0.31, 1.37, 0.14)  Wave  \n2   (0.32, 1.37, 0.14)  Wave  \n3   (0.36, 1.37, 0.14)  Wave  \n4   (0.42, 1.38, 0.16)  Wave  \n5   (0.41, 1.39, 0.16)  Wave  \n6   (0.40, 1.39, 0.16)  Wave  \n7   (0.35, 1.37, 0.16)  Wave  \n8   (0.32, 1.37, 0.15)  Wave  \n9   (0.27, 1.35, 0.14)  Wave  \n10  (0.26, 1.35, 0.15)  Wave  \n11  (0.27, 1.36, 0.15)  Wave  \n12  (0.32, 1.37, 0.17)  Wave  \n13  (0.37, 1.37, 0.19)  Wave  \n14  (0.39, 1.36, 0.19)  Wave  \n15  (0.38, 1.36, 0.19)  Wave  \n16  (0.34, 1.36, 0.18)  Wave  \n17  (0.30, 1.35, 0.17)  Wave  \n18  (0.26, 1.35, 0.15)  Wave  \n19  (0.24, 1.34, 0.15)  Wave  \n\n[20 rows x 22 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>WRIST</th>\n      <th>THUMB_CMC</th>\n      <th>THUMB_MCP</th>\n      <th>THUMB_IP</th>\n      <th>THUMB_TIP</th>\n      <th>INDEX_FINGER_MCP</th>\n      <th>INDEX_FINGER_PIP</th>\n      <th>INDEX_FINGER_DIP</th>\n      <th>INDEX_FINGER_TIP</th>\n      <th>MIDDLE_FINGER_MCP</th>\n      <th>...</th>\n      <th>MIDDLE_FINGER_TIP</th>\n      <th>RING_FINGER_MCP</th>\n      <th>RING_FINGER_PIP</th>\n      <th>RING_FINGER_DIP</th>\n      <th>RING_FINGER_TIP</th>\n      <th>PINKY_MCP</th>\n      <th>PINKY_PIP</th>\n      <th>PINKY_DIP</th>\n      <th>PINKY_TIP</th>\n      <th>LABEL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(0.35, 1.23, 0.11)</td>\n      <td>(0.32, 1.24, 0.11)</td>\n      <td>(0.27, 1.26, 0.11)</td>\n      <td>(0.25, 1.28, 0.11)</td>\n      <td>(0.25, 1.29, 0.11)</td>\n      <td>(0.29, 1.31, 0.11)</td>\n      <td>(0.27, 1.33, 0.12)</td>\n      <td>(0.26, 1.35, 0.12)</td>\n      <td>(0.25, 1.36, 0.13)</td>\n      <td>(0.30, 1.31, 0.12)</td>\n      <td>...</td>\n      <td>(0.27, 1.39, 0.13)</td>\n      <td>(0.32, 1.31, 0.13)</td>\n      <td>(0.31, 1.35, 0.13)</td>\n      <td>(0.31, 1.37, 0.14)</td>\n      <td>(0.30, 1.38, 0.14)</td>\n      <td>(0.33, 1.31, 0.14)</td>\n      <td>(0.33, 1.33, 0.14)</td>\n      <td>(0.33, 1.35, 0.14)</td>\n      <td>(0.33, 1.37, 0.14)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(0.34, 1.24, 0.11)</td>\n      <td>(0.31, 1.24, 0.11)</td>\n      <td>(0.27, 1.26, 0.10)</td>\n      <td>(0.25, 1.27, 0.10)</td>\n      <td>(0.24, 1.29, 0.10)</td>\n      <td>(0.28, 1.31, 0.11)</td>\n      <td>(0.26, 1.33, 0.11)</td>\n      <td>(0.24, 1.34, 0.11)</td>\n      <td>(0.23, 1.36, 0.12)</td>\n      <td>(0.29, 1.31, 0.11)</td>\n      <td>...</td>\n      <td>(0.25, 1.38, 0.12)</td>\n      <td>(0.31, 1.32, 0.12)</td>\n      <td>(0.30, 1.35, 0.13)</td>\n      <td>(0.29, 1.37, 0.13)</td>\n      <td>(0.28, 1.38, 0.13)</td>\n      <td>(0.32, 1.31, 0.13)</td>\n      <td>(0.32, 1.34, 0.13)</td>\n      <td>(0.31, 1.36, 0.14)</td>\n      <td>(0.31, 1.37, 0.14)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(0.34, 1.24, 0.11)</td>\n      <td>(0.31, 1.25, 0.11)</td>\n      <td>(0.27, 1.27, 0.10)</td>\n      <td>(0.25, 1.29, 0.10)</td>\n      <td>(0.24, 1.30, 0.11)</td>\n      <td>(0.28, 1.31, 0.11)</td>\n      <td>(0.27, 1.34, 0.11)</td>\n      <td>(0.25, 1.35, 0.12)</td>\n      <td>(0.24, 1.37, 0.12)</td>\n      <td>(0.30, 1.32, 0.11)</td>\n      <td>...</td>\n      <td>(0.26, 1.39, 0.13)</td>\n      <td>(0.31, 1.32, 0.12)</td>\n      <td>(0.30, 1.36, 0.13)</td>\n      <td>(0.29, 1.38, 0.13)</td>\n      <td>(0.29, 1.39, 0.14)</td>\n      <td>(0.33, 1.32, 0.14)</td>\n      <td>(0.33, 1.35, 0.14)</td>\n      <td>(0.32, 1.36, 0.14)</td>\n      <td>(0.32, 1.37, 0.14)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(0.37, 1.24, 0.11)</td>\n      <td>(0.35, 1.25, 0.12)</td>\n      <td>(0.30, 1.27, 0.11)</td>\n      <td>(0.29, 1.28, 0.11)</td>\n      <td>(0.28, 1.30, 0.11)</td>\n      <td>(0.32, 1.31, 0.11)</td>\n      <td>(0.30, 1.33, 0.11)</td>\n      <td>(0.29, 1.35, 0.12)</td>\n      <td>(0.28, 1.36, 0.13)</td>\n      <td>(0.33, 1.32, 0.11)</td>\n      <td>...</td>\n      <td>(0.30, 1.39, 0.13)</td>\n      <td>(0.35, 1.32, 0.12)</td>\n      <td>(0.34, 1.35, 0.13)</td>\n      <td>(0.33, 1.37, 0.13)</td>\n      <td>(0.33, 1.38, 0.14)</td>\n      <td>(0.36, 1.32, 0.13)</td>\n      <td>(0.37, 1.34, 0.13)</td>\n      <td>(0.37, 1.36, 0.13)</td>\n      <td>(0.36, 1.37, 0.14)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(0.39, 1.25, 0.12)</td>\n      <td>(0.36, 1.27, 0.13)</td>\n      <td>(0.33, 1.30, 0.13)</td>\n      <td>(0.31, 1.32, 0.13)</td>\n      <td>(0.31, 1.33, 0.13)</td>\n      <td>(0.36, 1.33, 0.13)</td>\n      <td>(0.35, 1.36, 0.13)</td>\n      <td>(0.34, 1.38, 0.14)</td>\n      <td>(0.34, 1.39, 0.15)</td>\n      <td>(0.37, 1.34, 0.13)</td>\n      <td>...</td>\n      <td>(0.36, 1.41, 0.15)</td>\n      <td>(0.39, 1.33, 0.14)</td>\n      <td>(0.39, 1.37, 0.14)</td>\n      <td>(0.39, 1.38, 0.15)</td>\n      <td>(0.39, 1.40, 0.16)</td>\n      <td>(0.40, 1.32, 0.14)</td>\n      <td>(0.41, 1.35, 0.14)</td>\n      <td>(0.42, 1.37, 0.15)</td>\n      <td>(0.42, 1.38, 0.16)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>(0.38, 1.27, 0.13)</td>\n      <td>(0.35, 1.29, 0.13)</td>\n      <td>(0.33, 1.32, 0.14)</td>\n      <td>(0.31, 1.34, 0.14)</td>\n      <td>(0.31, 1.35, 0.15)</td>\n      <td>(0.35, 1.35, 0.14)</td>\n      <td>(0.34, 1.38, 0.15)</td>\n      <td>(0.34, 1.40, 0.16)</td>\n      <td>(0.34, 1.41, 0.16)</td>\n      <td>(0.37, 1.35, 0.14)</td>\n      <td>...</td>\n      <td>(0.36, 1.42, 0.17)</td>\n      <td>(0.38, 1.35, 0.15)</td>\n      <td>(0.39, 1.38, 0.15)</td>\n      <td>(0.39, 1.40, 0.16)</td>\n      <td>(0.39, 1.41, 0.17)</td>\n      <td>(0.40, 1.34, 0.15)</td>\n      <td>(0.41, 1.36, 0.15)</td>\n      <td>(0.41, 1.38, 0.16)</td>\n      <td>(0.41, 1.39, 0.16)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>(0.37, 1.27, 0.12)</td>\n      <td>(0.34, 1.28, 0.13)</td>\n      <td>(0.31, 1.31, 0.14)</td>\n      <td>(0.30, 1.33, 0.14)</td>\n      <td>(0.30, 1.35, 0.15)</td>\n      <td>(0.34, 1.35, 0.14)</td>\n      <td>(0.33, 1.38, 0.15)</td>\n      <td>(0.33, 1.40, 0.16)</td>\n      <td>(0.32, 1.41, 0.16)</td>\n      <td>(0.35, 1.35, 0.14)</td>\n      <td>...</td>\n      <td>(0.35, 1.42, 0.17)</td>\n      <td>(0.37, 1.35, 0.15)</td>\n      <td>(0.38, 1.38, 0.15)</td>\n      <td>(0.38, 1.40, 0.16)</td>\n      <td>(0.38, 1.41, 0.17)</td>\n      <td>(0.39, 1.34, 0.15)</td>\n      <td>(0.40, 1.36, 0.15)</td>\n      <td>(0.40, 1.38, 0.16)</td>\n      <td>(0.40, 1.39, 0.16)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>(0.34, 1.25, 0.12)</td>\n      <td>(0.31, 1.26, 0.12)</td>\n      <td>(0.27, 1.29, 0.13)</td>\n      <td>(0.26, 1.30, 0.13)</td>\n      <td>(0.25, 1.32, 0.14)</td>\n      <td>(0.30, 1.33, 0.13)</td>\n      <td>(0.28, 1.35, 0.14)</td>\n      <td>(0.28, 1.37, 0.14)</td>\n      <td>(0.27, 1.38, 0.15)</td>\n      <td>(0.31, 1.33, 0.13)</td>\n      <td>...</td>\n      <td>(0.30, 1.40, 0.15)</td>\n      <td>(0.33, 1.33, 0.14)</td>\n      <td>(0.33, 1.36, 0.14)</td>\n      <td>(0.33, 1.38, 0.15)</td>\n      <td>(0.32, 1.39, 0.16)</td>\n      <td>(0.34, 1.32, 0.14)</td>\n      <td>(0.35, 1.34, 0.14)</td>\n      <td>(0.35, 1.36, 0.15)</td>\n      <td>(0.35, 1.37, 0.16)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(0.33, 1.24, 0.11)</td>\n      <td>(0.30, 1.25, 0.12)</td>\n      <td>(0.26, 1.27, 0.12)</td>\n      <td>(0.24, 1.29, 0.12)</td>\n      <td>(0.24, 1.30, 0.13)</td>\n      <td>(0.27, 1.31, 0.12)</td>\n      <td>(0.26, 1.34, 0.13)</td>\n      <td>(0.25, 1.36, 0.13)</td>\n      <td>(0.24, 1.37, 0.14)</td>\n      <td>(0.29, 1.32, 0.13)</td>\n      <td>...</td>\n      <td>(0.27, 1.39, 0.14)</td>\n      <td>(0.31, 1.32, 0.14)</td>\n      <td>(0.30, 1.35, 0.14)</td>\n      <td>(0.30, 1.37, 0.14)</td>\n      <td>(0.29, 1.38, 0.15)</td>\n      <td>(0.32, 1.31, 0.14)</td>\n      <td>(0.32, 1.34, 0.14)</td>\n      <td>(0.32, 1.35, 0.15)</td>\n      <td>(0.32, 1.37, 0.15)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>(0.31, 1.23, 0.12)</td>\n      <td>(0.28, 1.24, 0.11)</td>\n      <td>(0.24, 1.25, 0.10)</td>\n      <td>(0.22, 1.27, 0.10)</td>\n      <td>(0.21, 1.28, 0.11)</td>\n      <td>(0.24, 1.30, 0.11)</td>\n      <td>(0.23, 1.32, 0.11)</td>\n      <td>(0.21, 1.33, 0.11)</td>\n      <td>(0.20, 1.34, 0.12)</td>\n      <td>(0.26, 1.30, 0.11)</td>\n      <td>...</td>\n      <td>(0.22, 1.36, 0.12)</td>\n      <td>(0.27, 1.30, 0.13)</td>\n      <td>(0.26, 1.33, 0.13)</td>\n      <td>(0.25, 1.35, 0.13)</td>\n      <td>(0.25, 1.36, 0.14)</td>\n      <td>(0.29, 1.30, 0.14)</td>\n      <td>(0.28, 1.32, 0.14)</td>\n      <td>(0.28, 1.34, 0.14)</td>\n      <td>(0.27, 1.35, 0.14)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>(0.31, 1.24, 0.12)</td>\n      <td>(0.28, 1.24, 0.12)</td>\n      <td>(0.24, 1.25, 0.10)</td>\n      <td>(0.21, 1.26, 0.10)</td>\n      <td>(0.20, 1.27, 0.11)</td>\n      <td>(0.24, 1.29, 0.11)</td>\n      <td>(0.22, 1.31, 0.11)</td>\n      <td>(0.21, 1.32, 0.11)</td>\n      <td>(0.20, 1.33, 0.12)</td>\n      <td>(0.25, 1.30, 0.11)</td>\n      <td>...</td>\n      <td>(0.21, 1.36, 0.12)</td>\n      <td>(0.27, 1.31, 0.13)</td>\n      <td>(0.25, 1.33, 0.13)</td>\n      <td>(0.24, 1.35, 0.13)</td>\n      <td>(0.24, 1.36, 0.14)</td>\n      <td>(0.28, 1.30, 0.14)</td>\n      <td>(0.28, 1.33, 0.14)</td>\n      <td>(0.27, 1.34, 0.14)</td>\n      <td>(0.26, 1.35, 0.15)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>(0.31, 1.24, 0.13)</td>\n      <td>(0.29, 1.24, 0.12)</td>\n      <td>(0.24, 1.26, 0.11)</td>\n      <td>(0.22, 1.27, 0.11)</td>\n      <td>(0.21, 1.28, 0.11)</td>\n      <td>(0.25, 1.30, 0.11)</td>\n      <td>(0.23, 1.32, 0.11)</td>\n      <td>(0.22, 1.33, 0.12)</td>\n      <td>(0.21, 1.34, 0.13)</td>\n      <td>(0.26, 1.31, 0.12)</td>\n      <td>...</td>\n      <td>(0.22, 1.36, 0.13)</td>\n      <td>(0.28, 1.31, 0.13)</td>\n      <td>(0.26, 1.34, 0.13)</td>\n      <td>(0.25, 1.36, 0.14)</td>\n      <td>(0.25, 1.37, 0.14)</td>\n      <td>(0.29, 1.31, 0.15)</td>\n      <td>(0.28, 1.33, 0.15)</td>\n      <td>(0.28, 1.35, 0.15)</td>\n      <td>(0.27, 1.36, 0.15)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>(0.33, 1.26, 0.13)</td>\n      <td>(0.30, 1.26, 0.13)</td>\n      <td>(0.26, 1.28, 0.13)</td>\n      <td>(0.25, 1.30, 0.13)</td>\n      <td>(0.24, 1.31, 0.14)</td>\n      <td>(0.28, 1.32, 0.13)</td>\n      <td>(0.27, 1.35, 0.14)</td>\n      <td>(0.26, 1.36, 0.14)</td>\n      <td>(0.25, 1.37, 0.15)</td>\n      <td>(0.29, 1.33, 0.14)</td>\n      <td>...</td>\n      <td>(0.27, 1.39, 0.16)</td>\n      <td>(0.31, 1.33, 0.15)</td>\n      <td>(0.31, 1.36, 0.15)</td>\n      <td>(0.30, 1.38, 0.16)</td>\n      <td>(0.29, 1.39, 0.17)</td>\n      <td>(0.33, 1.32, 0.16)</td>\n      <td>(0.33, 1.35, 0.16)</td>\n      <td>(0.32, 1.36, 0.17)</td>\n      <td>(0.32, 1.37, 0.17)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>(0.35, 1.26, 0.13)</td>\n      <td>(0.32, 1.27, 0.14)</td>\n      <td>(0.29, 1.30, 0.15)</td>\n      <td>(0.28, 1.32, 0.15)</td>\n      <td>(0.27, 1.33, 0.16)</td>\n      <td>(0.31, 1.34, 0.15)</td>\n      <td>(0.31, 1.37, 0.15)</td>\n      <td>(0.30, 1.38, 0.16)</td>\n      <td>(0.30, 1.40, 0.17)</td>\n      <td>(0.33, 1.34, 0.15)</td>\n      <td>...</td>\n      <td>(0.33, 1.41, 0.18)</td>\n      <td>(0.35, 1.34, 0.16)</td>\n      <td>(0.35, 1.37, 0.17)</td>\n      <td>(0.35, 1.38, 0.18)</td>\n      <td>(0.35, 1.39, 0.18)</td>\n      <td>(0.36, 1.33, 0.16)</td>\n      <td>(0.37, 1.35, 0.17)</td>\n      <td>(0.37, 1.36, 0.18)</td>\n      <td>(0.37, 1.37, 0.19)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>(0.35, 1.26, 0.13)</td>\n      <td>(0.32, 1.27, 0.14)</td>\n      <td>(0.30, 1.31, 0.15)</td>\n      <td>(0.29, 1.33, 0.16)</td>\n      <td>(0.29, 1.34, 0.17)</td>\n      <td>(0.33, 1.34, 0.15)</td>\n      <td>(0.33, 1.37, 0.16)</td>\n      <td>(0.33, 1.38, 0.17)</td>\n      <td>(0.32, 1.39, 0.18)</td>\n      <td>(0.34, 1.34, 0.16)</td>\n      <td>...</td>\n      <td>(0.35, 1.40, 0.19)</td>\n      <td>(0.36, 1.33, 0.16)</td>\n      <td>(0.37, 1.36, 0.17)</td>\n      <td>(0.37, 1.38, 0.18)</td>\n      <td>(0.37, 1.39, 0.19)</td>\n      <td>(0.37, 1.32, 0.16)</td>\n      <td>(0.38, 1.34, 0.17)</td>\n      <td>(0.39, 1.35, 0.18)</td>\n      <td>(0.39, 1.36, 0.19)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>(0.34, 1.25, 0.13)</td>\n      <td>(0.31, 1.27, 0.14)</td>\n      <td>(0.29, 1.31, 0.15)</td>\n      <td>(0.28, 1.33, 0.16)</td>\n      <td>(0.28, 1.34, 0.17)</td>\n      <td>(0.32, 1.34, 0.15)</td>\n      <td>(0.32, 1.37, 0.16)</td>\n      <td>(0.32, 1.38, 0.17)</td>\n      <td>(0.31, 1.39, 0.18)</td>\n      <td>(0.34, 1.34, 0.15)</td>\n      <td>...</td>\n      <td>(0.34, 1.40, 0.19)</td>\n      <td>(0.35, 1.33, 0.16)</td>\n      <td>(0.36, 1.36, 0.17)</td>\n      <td>(0.36, 1.37, 0.18)</td>\n      <td>(0.36, 1.38, 0.19)</td>\n      <td>(0.36, 1.32, 0.16)</td>\n      <td>(0.37, 1.34, 0.17)</td>\n      <td>(0.38, 1.35, 0.18)</td>\n      <td>(0.38, 1.36, 0.19)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>(0.32, 1.25, 0.13)</td>\n      <td>(0.29, 1.27, 0.13)</td>\n      <td>(0.26, 1.29, 0.14)</td>\n      <td>(0.25, 1.31, 0.15)</td>\n      <td>(0.24, 1.33, 0.16)</td>\n      <td>(0.29, 1.33, 0.15)</td>\n      <td>(0.28, 1.36, 0.15)</td>\n      <td>(0.28, 1.38, 0.16)</td>\n      <td>(0.27, 1.39, 0.17)</td>\n      <td>(0.30, 1.33, 0.15)</td>\n      <td>...</td>\n      <td>(0.29, 1.40, 0.18)</td>\n      <td>(0.32, 1.33, 0.16)</td>\n      <td>(0.32, 1.36, 0.16)</td>\n      <td>(0.32, 1.37, 0.17)</td>\n      <td>(0.32, 1.39, 0.18)</td>\n      <td>(0.33, 1.32, 0.16)</td>\n      <td>(0.34, 1.34, 0.17)</td>\n      <td>(0.34, 1.35, 0.18)</td>\n      <td>(0.34, 1.36, 0.18)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>(0.30, 1.24, 0.12)</td>\n      <td>(0.27, 1.25, 0.13)</td>\n      <td>(0.23, 1.27, 0.12)</td>\n      <td>(0.22, 1.29, 0.13)</td>\n      <td>(0.22, 1.31, 0.14)</td>\n      <td>(0.25, 1.31, 0.13)</td>\n      <td>(0.24, 1.34, 0.14)</td>\n      <td>(0.24, 1.35, 0.15)</td>\n      <td>(0.23, 1.36, 0.15)</td>\n      <td>(0.27, 1.32, 0.14)</td>\n      <td>...</td>\n      <td>(0.25, 1.38, 0.16)</td>\n      <td>(0.28, 1.31, 0.14)</td>\n      <td>(0.28, 1.34, 0.15)</td>\n      <td>(0.28, 1.36, 0.16)</td>\n      <td>(0.27, 1.37, 0.16)</td>\n      <td>(0.30, 1.30, 0.15)</td>\n      <td>(0.30, 1.33, 0.16)</td>\n      <td>(0.30, 1.34, 0.16)</td>\n      <td>(0.30, 1.35, 0.17)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>(0.29, 1.23, 0.12)</td>\n      <td>(0.26, 1.23, 0.11)</td>\n      <td>(0.22, 1.25, 0.11)</td>\n      <td>(0.20, 1.26, 0.11)</td>\n      <td>(0.19, 1.27, 0.12)</td>\n      <td>(0.23, 1.29, 0.11)</td>\n      <td>(0.22, 1.31, 0.11)</td>\n      <td>(0.20, 1.33, 0.12)</td>\n      <td>(0.19, 1.34, 0.13)</td>\n      <td>(0.24, 1.30, 0.12)</td>\n      <td>...</td>\n      <td>(0.21, 1.36, 0.13)</td>\n      <td>(0.26, 1.30, 0.13)</td>\n      <td>(0.25, 1.33, 0.13)</td>\n      <td>(0.24, 1.35, 0.14)</td>\n      <td>(0.24, 1.36, 0.14)</td>\n      <td>(0.27, 1.29, 0.14)</td>\n      <td>(0.27, 1.32, 0.14)</td>\n      <td>(0.27, 1.33, 0.15)</td>\n      <td>(0.26, 1.35, 0.15)</td>\n      <td>Wave</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>(0.28, 1.22, 0.11)</td>\n      <td>(0.25, 1.22, 0.11)</td>\n      <td>(0.21, 1.23, 0.10)</td>\n      <td>(0.19, 1.25, 0.10)</td>\n      <td>(0.18, 1.26, 0.10)</td>\n      <td>(0.22, 1.28, 0.10)</td>\n      <td>(0.20, 1.30, 0.10)</td>\n      <td>(0.19, 1.31, 0.11)</td>\n      <td>(0.18, 1.31, 0.11)</td>\n      <td>(0.23, 1.29, 0.11)</td>\n      <td>...</td>\n      <td>(0.19, 1.34, 0.12)</td>\n      <td>(0.24, 1.29, 0.12)</td>\n      <td>(0.23, 1.32, 0.12)</td>\n      <td>(0.22, 1.33, 0.13)</td>\n      <td>(0.21, 1.34, 0.13)</td>\n      <td>(0.26, 1.29, 0.14)</td>\n      <td>(0.25, 1.31, 0.14)</td>\n      <td>(0.25, 1.33, 0.14)</td>\n      <td>(0.24, 1.34, 0.15)</td>\n      <td>Wave</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows × 22 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_custom_data = pd.read_csv(\"data/graphdata/custom_data/WinkenDaumenHochDataset.csv\")\n",
    "df_custom_data['LABEL'] = df_custom_data['LABEL'].replace({'Winken': 'Wave', 'DaumenHoch': 'ThumbsUp'})\n",
    "\n",
    "\n",
    "unique_gestures_start_indices = df_custom_data[df_custom_data['Unnamed: 0'] == 0].index\n",
    "\n",
    "train_start_indices, test_start_indices = train_test_split(unique_gestures_start_indices, test_size=0.3, random_state=42)\n",
    "\n",
    "def extract_gesture_data(df, start_indices):\n",
    "    shuffled_indices = sklearn.utils.shuffle(start_indices, random_state=42)\n",
    "    frames = []\n",
    "    for idx in shuffled_indices:\n",
    "        frames.extend(range(idx, idx + 32))\n",
    "    return df.loc[frames]\n",
    "\n",
    "df_train = extract_gesture_data(df_custom_data, train_start_indices)\n",
    "df_test = extract_gesture_data(df_custom_data, test_start_indices)\n",
    "\n",
    "df_train = df_train.drop(\"Unnamed: 0\", axis=1).reset_index(drop=True)\n",
    "df_test = df_test.drop(\"Unnamed: 0\", axis=1).reset_index(drop=True)\n",
    "\n",
    "print(\"[INFO] TRAIN DATA DISTRIBUTION\")\n",
    "print(df_train[\"LABEL\"].value_counts())\n",
    "print(\"[INFO] TEST DATA DISTRIBUTION\")\n",
    "print(df_test[\"LABEL\"].value_counts())\n",
    "\n",
    "df_train.head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T11:06:00.890788800Z",
     "start_time": "2024-01-17T11:06:00.779845300Z"
    }
   },
   "id": "5f9ab104b5f11b42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finetuning first try: remove all other classes except the classes we want to have in our model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e22c6215d8a5efd"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4320 entries, 0 to 4319\n",
      "Data columns (total 22 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   WRIST              4320 non-null   object\n",
      " 1   THUMB_CMC          4320 non-null   object\n",
      " 2   THUMB_MCP          4320 non-null   object\n",
      " 3   THUMB_IP           4320 non-null   object\n",
      " 4   THUMB_TIP          4320 non-null   object\n",
      " 5   INDEX_FINGER_MCP   4320 non-null   object\n",
      " 6   INDEX_FINGER_PIP   4320 non-null   object\n",
      " 7   INDEX_FINGER_DIP   4320 non-null   object\n",
      " 8   INDEX_FINGER_TIP   4320 non-null   object\n",
      " 9   MIDDLE_FINGER_MCP  4320 non-null   object\n",
      " 10  MIDDLE_FINGER_PIP  4320 non-null   object\n",
      " 11  MIDDLE_FINGER_DIP  4320 non-null   object\n",
      " 12  MIDDLE_FINGER_TIP  4320 non-null   object\n",
      " 13  RING_FINGER_MCP    4320 non-null   object\n",
      " 14  RING_FINGER_PIP    4320 non-null   object\n",
      " 15  RING_FINGER_DIP    4320 non-null   object\n",
      " 16  RING_FINGER_TIP    4320 non-null   object\n",
      " 17  PINKY_MCP          4320 non-null   object\n",
      " 18  PINKY_PIP          4320 non-null   object\n",
      " 19  PINKY_DIP          4320 non-null   object\n",
      " 20  PINKY_TIP          4320 non-null   object\n",
      " 21  LABEL              4320 non-null   object\n",
      "dtypes: object(22)\n",
      "memory usage: 742.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#df_train = df_train[df_train[\"LABEL\"].isin(CFG.classes_fine_tuning)]\n",
    "#df_val = df_val[df_val[\"LABEL\"].isin(CFG.classes_fine_tuning)]\n",
    "df_train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T11:06:02.890783900Z",
     "start_time": "2024-01-17T11:06:02.874772800Z"
    }
   },
   "id": "8dc0787fc2d358cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare train and validation data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29aab5179473bf86"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TRAIN DATA DISTRIBUTION\n",
      "LABEL\n",
      "Wave        2208\n",
      "ThumbsUp    2112\n",
      "Name: count, dtype: int64\n",
      "[INFO] VALIDATION DATA DISTRIBUTION\n",
      "LABEL\n",
      "Wave        1120\n",
      "ThumbsUp     736\n",
      "Name: count, dtype: int64\n",
      "df_to_numpy: Original labels: [['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']]\n",
      "One-hot encoded labels: [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "df_to_numpy: Original labels: [['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['ThumbsUp']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']\n",
      " ['Wave']]\n",
      "One-hot encoded labels: [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "[INFO] TRAINING ON 4320 DATAPOINTS\n",
      "[INFO] VALIDATION ON 1856 DATAPOINTS\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] TRAIN DATA DISTRIBUTION\")\n",
    "print(df_train[\"LABEL\"].value_counts())\n",
    "print(\"[INFO] VALIDATION DATA DISTRIBUTION\")\n",
    "print(df_test[\"LABEL\"].value_counts())\n",
    "\n",
    "train_numpy = df_to_numpy(df_train)\n",
    "val_numpy = df_to_numpy(df_test)\n",
    "\n",
    "train_set = HandPoseDatasetNumpy(train_numpy)\n",
    "val_set = HandPoseDatasetNumpy(val_numpy)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=CFG.batch_size, drop_last=True, shuffle=False, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=CFG.batch_size, drop_last=True, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(f\"[INFO] TRAINING ON {len(train_set)} DATAPOINTS\")\n",
    "print(f\"[INFO] VALIDATION ON {len(val_set)} DATAPOINTS\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T11:06:04.647281100Z",
     "start_time": "2024-01-17T11:06:02.890783900Z"
    }
   },
   "id": "b415ca84109554c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fine tuning config and preparation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9db523fbb4200c5d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "optimizer_base = torch.optim.Adam\n",
    "optimizer = SAM.SAM(model.parameters(), optimizer_base, lr=CFG.lr, rho=0.5, adaptive=True)\n",
    "criterion = loss.FocalLoss()\n",
    "\n",
    "writer = SummaryWriter(f'fine_tuned_models_occluded_hand_detection/runs/{CFG.experiment_name}')\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=CFG.min_lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T11:06:04.665418700Z",
     "start_time": "2024-01-17T11:06:04.647281100Z"
    }
   },
   "id": "ad310ad7e2983f47"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_func(model, data_loader, criterion, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    iters = len(data_loader)\n",
    "    global_step = epoch * len(data_loader)\n",
    "    preds = []\n",
    "    groundtruth = []\n",
    "    t0 = time.time()\n",
    "    loss_total = 0\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        labels = labels.cuda().long()\n",
    "        inputs = inputs.cuda().float()\n",
    "\n",
    "        last_label = labels[:, -1, :]\n",
    "        last_label = torch.argmax(last_label, 1)\n",
    "        \n",
    "        #print(\"TRAIN Labels shape:\", labels.shape)\n",
    "        #print(\"Last label shape:\", last_label.shape)\n",
    "        #print(\"First 10 labels:\", labels[:10])\n",
    "        #print(\"First 10 last labels:\", last_label[:10])\n",
    "\n",
    "        model.zero_grad()\n",
    "        last_out = model(inputs)\n",
    "\n",
    "        # first forward-backward pass\n",
    "        loss = criterion(last_out, last_label)\n",
    "        loss.backward()\n",
    "\n",
    "        if CFG.sam:\n",
    "            optimizer.first_step(zero_grad=True)  #\n",
    "\n",
    "            # second forward-backward pass\n",
    "            criterion(model(inputs), last_label).backward()  #\n",
    "            optimizer.second_step(zero_grad=True)  #\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        preds.append(last_out.cpu().detach().numpy())\n",
    "        groundtruth.append(last_label.cpu().detach().numpy())\n",
    "\n",
    "        loss_total += loss\n",
    "        global_step += 1\n",
    "        writer.add_scalar('Loss/Train', loss, global_step)\n",
    "        writer.add_scalar('LR', current_lr, global_step)\n",
    "\n",
    "        if i % CFG.print_freq == 1 or i == iters - 1:\n",
    "            t1 = time.time()\n",
    "            print(\n",
    "                f\"[TRAIN] Epoch: {epoch}/{CFG.epochs} | Iteration: {i}/{iters} | Loss: {loss_total / i} | LR: {current_lr} | ETA: {((t1 - t0) / i * iters) - (t1 - t0)}s\")\n",
    "\n",
    "    return loss_total, np.argmax(preds, axis=2).flatten(), np.array(groundtruth).flatten()\n",
    "\n",
    "\n",
    "def eval_func(model, criterion, data_loader, epoch):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    groundtruth = []\n",
    "    t0 = time.time()\n",
    "    loss_total = 0\n",
    "    global_step = len(train_loader) * epoch\n",
    "    iters = len(data_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(data_loader):\n",
    "            labels = labels.cuda().long()\n",
    "            inputs = inputs.cuda().float()\n",
    "\n",
    "            last_label = labels[:, -1, :]\n",
    "            last_label = torch.argmax(last_label, 1)\n",
    "\n",
    "            #print(\"EVAL Labels shape:\", labels.shape)\n",
    "            #print(\"Last label shape:\", last_label.shape)\n",
    "            #print(\"First 10 labels:\", labels[:10])\n",
    "           # print(\"First 10 last labels:\", last_label[:10])\n",
    "\n",
    "            last_out = model(inputs)\n",
    "            loss = criterion(last_out, last_label)\n",
    "\n",
    "            preds.append(last_out.cpu().detach().numpy())\n",
    "            groundtruth.append(last_label.cpu().detach().numpy())\n",
    "            loss_total += loss\n",
    "\n",
    "            if i % CFG.print_freq == 1 or i == iters - 1:\n",
    "                t1 = time.time()\n",
    "                print(\n",
    "                    f\"[EVAL] Epoch: {epoch}/{CFG.epochs} | Iteration: {i}/{iters} | Val-Loss: {loss_total / i} | ETA: {((t1 - t0) / i * iters) - (t1 - t0)}s\")\n",
    "\n",
    "    writer.add_scalar('Loss/Validation', loss_total / i, global_step)\n",
    "    return loss_total, np.argmax(preds, axis=2).flatten(), np.array(groundtruth).flatten()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T11:06:04.678924500Z",
     "start_time": "2024-01-17T11:06:04.665418700Z"
    }
   },
   "id": "f58e55abe0c3428f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Start Fine tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe8f85ca1658b36b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch: 0/500 | Iteration: 1/270 | Loss: 8.629823684692383 | LR: 0.0001 | ETA: 302.8565900325775s\n",
      "[TRAIN] Epoch: 0/500 | Iteration: 101/270 | Loss: 0.3037799298763275 | LR: 0.0001 | ETA: 39.7674713441641s\n",
      "[TRAIN] Epoch: 0/500 | Iteration: 201/270 | Loss: 0.15346181392669678 | LR: 0.0001 | ETA: 15.759114187155191s\n",
      "[TRAIN] Epoch: 0/500 | Iteration: 269/270 | Loss: 0.11487846076488495 | LR: 0.0001 | ETA: 0.22659474500493104s\n",
      "[TRAIN] Training F1-Score 0.9627314814814815\n",
      "[EVAL] Epoch: 0/500 | Iteration: 1/116 | Val-Loss: 1.2539553040369356e-07 | ETA: 7.218263149261475s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smujan\\PycharmProjects\\Hand-Gesture-Recognition-in-manual-assembly-tasks-using-GCN\\utils\\training_supervision.py:27: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "  plt.xlim(left=0, right=len(ave_grads))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EVAL] Epoch: 0/500 | Iteration: 101/116 | Val-Loss: 1.7672091416898184e-06 | ETA: 0.4525654622823887s\n",
      "[EVAL] Epoch: 0/500 | Iteration: 115/116 | Val-Loss: 1.689474288468773e-06 | ETA: 0.030030634092248043s\n",
      "[EVAL] Validation F1-Score Micro 1.0\n",
      "[EVAL] Validation F1-Score Macro 1.0\n",
      "[EVAL] Classification Report\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 48\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[EVAL] Classification Report\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m---> 48\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mclassification_report\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgt_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreds_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCFG\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclasses_fine_tuning\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdigits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;66;03m#print(classification_report(gt_val, preds_val, labels=[CFG.classes_fine_tuning], target_names=CFG.classes_fine_tuning, digits=3))\u001B[39;00m\n\u001B[0;32m     52\u001B[0m scheduler\u001B[38;5;241m.\u001B[39mstep(val_loss)  \u001B[38;5;66;03m#for reduce lr on plateau\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Hand-Gesture-Recognition-in-manual-assembly-tasks-using-GCN\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    208\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    210\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    211\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    212\u001B[0m         )\n\u001B[0;32m    213\u001B[0m     ):\n\u001B[1;32m--> 214\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    220\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    223\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    224\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Hand-Gesture-Recognition-in-manual-assembly-tasks-using-GCN\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2567\u001B[0m, in \u001B[0;36mclassification_report\u001B[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001B[0m\n\u001B[0;32m   2561\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   2562\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels size, \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m, does not match size of target_names, \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2563\u001B[0m                 \u001B[38;5;28mlen\u001B[39m(labels), \u001B[38;5;28mlen\u001B[39m(target_names)\n\u001B[0;32m   2564\u001B[0m             )\n\u001B[0;32m   2565\u001B[0m         )\n\u001B[0;32m   2566\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2567\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2568\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of classes, \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m, does not match size of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2569\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget_names, \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m. Try specifying the labels \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2570\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mlen\u001B[39m(labels), \u001B[38;5;28mlen\u001B[39m(target_names))\n\u001B[0;32m   2571\u001B[0m         )\n\u001B[0;32m   2572\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m target_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2573\u001B[0m     target_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m l \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m labels]\n",
      "\u001B[1;31mValueError\u001B[0m: Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 800x800 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, CFG.epochs + start_epoch):\n",
    "    global_step = len(train_loader) * epoch\n",
    "\n",
    "    #TRAIN\n",
    "    train_loss, preds_train, gt_train = train_func(model, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "    train_grad_flow_plot = training_supervision.get_plot_grad_flow(model)\n",
    "\n",
    "    f1_train = f1_score(gt_train, preds_train, average=\"micro\")\n",
    "    writer.add_scalar('Accuracy/Train', f1_train, global_step)\n",
    "    print(f\"[TRAIN] Training F1-Score {f1_train}\")\n",
    "\n",
    "    #Model Gradients\n",
    "    names, gradmean = training_supervision.get_model_grads(model)\n",
    "    _limits = np.array([float(i) for i in range(len(gradmean))])\n",
    "    _num = len(gradmean)\n",
    "    writer.add_histogram_raw(tag=\"ModelGrads/MeanGradientFlow\", min=0.0, max=0.5, num=_num,\n",
    "                             sum=gradmean.sum(), sum_squares=np.power(gradmean, 2).sum(), bucket_limits=_limits,\n",
    "                             bucket_counts=gradmean, global_step=global_step)\n",
    "\n",
    "    #VAL\n",
    "    val_loss, preds_val, gt_val = eval_func(model, criterion, val_loader, epoch)\n",
    "\n",
    "    f1_val_micro = f1_score(gt_val, preds_val, average=\"micro\")\n",
    "    f1_val_macro = f1_score(gt_val, preds_val, average=\"macro\")\n",
    "    writer.add_scalar('Accuracy/Validation/F1-Micro', f1_val_micro, global_step)\n",
    "    writer.add_scalar('Accuracy/Validation/F1-Macro', f1_val_macro, global_step)\n",
    "    print(f\"[EVAL] Validation F1-Score Micro {f1_val_micro}\")\n",
    "    print(f\"[EVAL] Validation F1-Score Macro {f1_val_macro}\")\n",
    "\n",
    "    #Conf Mat\n",
    "    cm = sklearn.metrics.confusion_matrix(gt_val, preds_val)\n",
    "    cm_plot = training_supervision.plot_confusion_matrix(cm, CFG.classes_fine_tuning)\n",
    "    writer.add_figure(\"Confusion Matrix/Validation\", cm_plot, global_step)\n",
    "\n",
    "    #Model Weights\n",
    "    names, params = training_supervision.get_model_weights(model)\n",
    "    for n, p in zip(names, params):\n",
    "        writer.add_histogram(f\"ModelWeights/{n}\", p, global_step)\n",
    "\n",
    "\n",
    "    #print(\"Erste paar wahre Labels:\", gt_val[:700])\n",
    "    #print(\"Erste paar Vorhersagen:\", preds_val[:700])\n",
    "    \n",
    "    print(\"[EVAL] Classification Report\")\n",
    "    #\n",
    "    print(classification_report(gt_val, preds_val, target_names=CFG.classes_fine_tuning, digits=3))\n",
    "    #print(classification_report(gt_val, preds_val, labels=[CFG.classes_fine_tuning], target_names=CFG.classes_fine_tuning, digits=3))\n",
    "\n",
    "\n",
    "    scheduler.step(val_loss)  #for reduce lr on plateau\n",
    "\n",
    "    import os\n",
    "    os.makedirs(f\"fine_tuned_models_occluded_hand_detection/{CFG.experiment_name}\", exist_ok=True)\n",
    "    PATH = f\"fine_tuned_models_occluded_hand_detection/{CFG.experiment_name}/f1{f1_val_micro}_valloss{val_loss}_epoch{epoch}.pth\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'f1_micro_val-score': f1_val_micro,\n",
    "    }, PATH)\n",
    "    print(f\"[INFO] MODEL SAVED to {PATH}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T11:07:12.303895100Z",
     "start_time": "2024-01-17T11:06:06.742555400Z"
    }
   },
   "id": "33de09290ba20fd5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### EVALUATION"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b7dbf2c276a5a8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import lstm, aagcn, stconv, SAM\n",
    "from data.handpose_dataset import HandPoseDatasetNumpy, df_to_numpy\n",
    "from data.get_data_from_csv import get_train_data, get_val_data\n",
    "from utils import training_supervision, adj_mat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-17T02:46:00.653730100Z"
    }
   },
   "id": "97e6fccd5ceb63fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loader = val_loader\n",
    "\n",
    "graph = aagcn.Graph(adj_mat.num_node, adj_mat.self_link, adj_mat.inward, adj_mat.outward, adj_mat.neighbor)\n",
    "model = aagcn.Model(num_class=CFG.num_classes_fine_tuning, num_point=21, num_person=1, graph=graph, drop_out=0.5, in_channels=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-17T02:46:00.655729300Z"
    }
   },
   "id": "b67cbc316a8e9492"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight=None, \n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob, \n",
    "            target_tensor, \n",
    "            weight=self.weight,\n",
    "            reduction = self.reduction\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "def eval_func(model, criterion, data_loader, epoch):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    groundtruth = []\n",
    "    t0 = time.time()\n",
    "    loss_total = 0\n",
    "    global_step = 0\n",
    "    iters = len(data_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(data_loader):\n",
    "            labels = labels.cuda().long()\n",
    "            inputs = inputs.cuda().float()\n",
    "\n",
    "            last_label = labels[:, -1, :]\n",
    "            last_label = torch.argmax(last_label, 1)\n",
    "\n",
    "            last_out = model(inputs)\n",
    "            loss = criterion(last_out, last_label)\n",
    "\n",
    "            preds.append(last_out.cpu().detach().numpy())\n",
    "            groundtruth.append(last_label.cpu().detach().numpy())\n",
    "            loss_total += loss\n",
    "\n",
    "            if i%CFG.print_freq == 1 or i == iters-1:\n",
    "                t1 = time.time()\n",
    "                print(f\"[EVAL] Iteration: {i}/{iters} | Val-Loss: {loss_total/i} | ETA: {((t1-t0)/i * iters) - (t1-t0)}s\")\n",
    "\n",
    "    return loss_total, np.array(preds),  np.array(groundtruth).flatten()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-17T02:46:00.658730300Z"
    }
   },
   "id": "4996a323b9bb7f5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL_PATH = \"fine_tuned_models_occluded_hand_detection/WaveThumbsUp_AAGCN_seqlen32_finetuned_SAM_joints1_joints2_/f11.0_valloss2.483986794032944e-09_epoch7.pth\"\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH)[\"model_state_dict\"])\n",
    "model.cuda()\n",
    "criterion = FocalLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-17T02:46:00.660730500Z"
    }
   },
   "id": "b36ca35f322a065"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_loss, preds_val, gt_val = eval_func(model, criterion, test_loader, 0)\n",
    "print(f\"[EVAL] VALIDATION LOSS MODEL {val_loss}\")\n",
    "print(classification_report(gt_val, np.argmax(preds_val, axis=2).flatten(), target_names=CFG.classes_fine_tuning, digits=4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-17T02:46:00.662731200Z"
    }
   },
   "id": "29204284d9bc2859"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T02:46:00.668732Z",
     "start_time": "2024-01-17T02:46:00.664731400Z"
    }
   },
   "id": "51e076219876f0d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
